import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt
from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation
from PIL import Image
import os
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# ğŸ’¡ ×˜×•×¢×Ÿ ××ª ××•×“×œ CLIPSeg
processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
model = CLIPSegForImageSegmentation.from_pretrained("CIDAS/clipseg-rd64-refined")

# ğŸ’¡ ×˜×•×¢×Ÿ ×ª××•× ×”
image_path = "train\M42441-1-E.jpg"
image = Image.open(image_path).convert("RGB")

# ğŸ’¡ ×˜×§×¡×˜ ×©××’×“×™×¨ ××” ×œ×–×”×•×ª
# prompt = ["scroll fragment"]  # ××¤×©×¨ ×œ×”×•×¡×™×£ ××™×œ×™× ××—×¨×•×ª ×›×“×™ ×œ×©×¤×¨ ×“×™×•×§
prompt = ["scroll fragment", "ancient manuscript", "parchment", "scroll", "handwritten text"]

# ××¢×‘×“ ××ª ×”×ª××•× ×” ×•×”×˜×§×¡×˜
inputs = processor(text=prompt, images=image, return_tensors="pt")

# ××§×‘×œ ××¡×™×›×” ×©×œ ×”××•×‘×™×™×§×˜ ×”××ª××™× ×œ×˜×§×¡×˜
with torch.no_grad():
    outputs = model(**inputs)
    mask = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()

# ğŸ’¡ ××¡× ×Ÿ ×•××’×“×™×¨ ×¡×£ ×œ××¡×™×›×” (Thresholding)
mask = (mask > 0.5).astype(np.uint8)  # ××¡× ×Ÿ ××–×•×¨×™× ×¢× ×”×¡×ª×‘×¨×•×ª ×’×‘×•×”×” ×‘×œ×‘×“

# ğŸ’¡ ×××™×¨ ××ª ×”××¡×™×›×” ×œ×ª×™×‘×•×ª ×—×•×¡××•×ª (Bounding Boxes) ×¢× OpenCV
contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
image_cv = np.array(image)

for contour in contours:
    x, y, w, h = cv2.boundingRect(contour)
    cv2.rectangle(image_cv, (x, y), (x + w, y + h), (255, 0, 0), 2)  # ×ª×™×‘×” ×›×—×•×œ×”

# ğŸ’¡ ×©××™×¨×ª ×”×ª×•×¦××”
cv2.imwrite(image_path.replace(".jpg", "_output2.jpg"), image_cv)
print("âœ… ×ª×™×•×’ ×”×•×©×œ×! ×”×ª××•× ×” × ×©××¨×” ×‘×©× ", os.path.realpath(image_path.replace(".jpg", "_output2.jpg")))

# ×”×¦×’×ª ×”×ª××•× ×” ×¢× ×”××¡×™×›×•×ª
plt.imshow(image_cv)
plt.axis("off")
plt.show()